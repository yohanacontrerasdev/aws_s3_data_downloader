{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Get the data\n",
    "\n",
    "**Download the data by executing the code below:**\n",
    "\n",
    "`Notes:`\n",
    "* This script will download all PDF files from an AWS S3 bucket, maintaining the directory structure, and store them in a DataFrame.\n",
    "* Ensure you have the necessary AWS credentials and configurations set in a .env file.\n",
    "* The script uses boto3 to interact with S3, pandas to handle the data, and re for string manipulation.\n",
    "* The script first downloads all PDF files, then filters these files to obtain those with the most recent and oldest years per company.\n",
    "* Finally, it extracts the content of the filtered PDFs using the LlamaParse library.\n",
    "* The extracted content is stored in a new DataFrame, which includes the PDF file names and their corresponding text content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:16.609619Z",
     "start_time": "2024-07-24T22:20:14.492759Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from src.plots import(\n",
    "  analyze_text,\n",
    "  analyze_sentiment,\n",
    "  generate_word_cloud,\n",
    "  plot_common_words,\n",
    "  display_ngrams_with_plot_side_by_side\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:34.151722Z",
     "start_time": "2024-07-24T22:20:16.611719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id be702afd-bc7c-4241-b329-14b452d83978\n",
      "Started parsing the file under job_id 89e8b081-7cc5-47f3-a9ba-01f8527c6f03\n"
     ]
    }
   ],
   "source": [
    "from src import data_utils\n",
    "\n",
    "# Call the function to download the PDFs\n",
    "filtered_pdfs_df = data_utils.download_pdfs_and_convert_to_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Normalize the data\n",
    "\n",
    "**Normalizing text is crucial for preparing data for further analysis, ensuring the text is consistent and easy to process by removing noise and standardizing the format.**\n",
    "\n",
    "- **Text Cleaning (`clean_text`):** Convert text to lowercase, remove unwanted characters such as punctuation, URLs, HTML tags, and digits.\n",
    "- **Expand Contractions (`expand_contractions`):** Replace contractions (e.g., \"can't\" to \"cannot\") using a predefined dictionary of contractions.\n",
    "- **Lemmatize Text (`lemmatize_text`):** Tokenize the text and apply lemmatization to convert words to their base form (e.g., \"running\" to \"run\").\n",
    "- **Remove Stopwords (`remove_stopwords`):** Tokenize the text and remove common stop words that do not contribute to the meaning (e.g., \"and\", \"the\").\n",
    "- **Normalize Corpus (`normalize_corpus`):** Combine text chunks into a single string if needed, apply text cleaning, contraction expansion, lemmatization, and stop word removal in sequence, save the cleaned and processed text to a `.txt` file with a specified prefix, and return the normalized text and the output file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:36.324178Z",
     "start_time": "2024-07-24T22:20:34.152956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/oem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/oem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src import text_normalizer\n",
    "\n",
    "# Text cleanup and normalization\n",
    "cleaned_text = text_normalizer.normalize_corpus(filtered_pdfs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Words in the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:36.475940Z",
     "start_time": "2024-07-24T22:20:36.325351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 4013\n",
      "Words in the vocabulary: ['american', 'nir2o2c09', '442', 'risk', 'needed', 'forecast', '2012', 'neuhaus', '2769', 'produce']\n"
     ]
    }
   ],
   "source": [
    "# Call the analyze_text function\n",
    "X, num_words, vocab_sample = analyze_text(cleaned_text)\n",
    "\n",
    "print(\"Number of words in the vocabulary:\", num_words)\n",
    "print(\"Words in the vocabulary:\", vocab_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sentiment of the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:36.723550Z",
     "start_time": "2024-07-24T22:20:36.477190Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment = analyze_sentiment(cleaned_text)\n",
    "print(f\"Sentiment of the text: Polarity={sentiment.polarity}, Subjectivity={sentiment.subjectivity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:37.636223Z",
     "start_time": "2024-07-24T22:20:36.724530Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_word_cloud(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Common Words Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:38.030739Z",
     "start_time": "2024-07-24T22:20:37.637532Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_common_words(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Top 10 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:38.410977Z",
     "start_time": "2024-07-24T22:20:38.032571Z"
    }
   },
   "outputs": [],
   "source": [
    "display_ngrams_with_plot_side_by_side(cleaned_text, n=2, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "In this stage, we have split the texts into fragments and vectorized them so that the machine learning models can understand them. We used the `CharacterTextSplitter` class from LangChain to divide the long texts into more manageable fragments, ensuring that each fragment retains enough context.\n",
    "\n",
    "After splitting the texts, we store the fragments in a new column of the DataFrame. Then, we use OpenAI embeddings to convert these text fragments into numerical vectors. Embeddings are numerical representations that capture the semantics and context of the texts.\n",
    "\n",
    "Finally, we store these vectors in a `VectorStore` using FAISS, a library for searching and storing large amounts of vectors. This will allow us to search and retrieve similar text fragments quickly and efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T22:20:43.450686Z",
     "start_time": "2024-07-24T22:20:38.412364Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from src import text_processing\n",
    "\n",
    "# Create and save the vectorstore\n",
    "text_processing.create_and_save_vectorstore(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ragas Evaluation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e313329e3043f687f52f676dbb9af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_precision</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.488095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_relevancy</th>\n",
       "      <td>0.787714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_correctness</th>\n",
       "      <td>0.654904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_similarity</th>\n",
       "      <td>0.926747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Result\n",
       "context_precision   1.000000\n",
       "faithfulness        0.488095\n",
       "answer_relevancy    0.787714\n",
       "context_recall      0.857143\n",
       "answer_correctness  0.654904\n",
       "answer_similarity   0.926747"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import ragas_utils\n",
    "\n",
    "#Evaluation\n",
    "result = ragas_utils.get_evaluation()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/Documentos/Github/aws_s3_data_downloader/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7 examples [00:00, 334.94 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_dataset = load_dataset('json', data_files='data.json')\n",
    "data = ragas_dataset['train']\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metrics=[\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 42/42 [00:13<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "try:\n",
    "    result = evaluate(\n",
    "        data,\n",
    "        metrics=metrics,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "    result\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation():\n",
    "    ragas_dataset = load_dataset('json', data_files='data.json')\n",
    "    data = ragas_dataset['train']\n",
    "    #print(\"-------- DATA ------------\")\n",
    "    #print(data)\n",
    "\n",
    "    # Metrics\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ]\n",
    "\n",
    "    # Evaluation\n",
    "    result = evaluate(\n",
    "    data,\n",
    "    metrics=metrics,\n",
    "    raise_exceptions=False\n",
    "    )\n",
    "\n",
    "    # Resultado Global\n",
    "    #print(result)\n",
    "    df = pd.DataFrame(result, index=[0])\n",
    "    res_df = df.transpose()\n",
    "    res_df.columns = [\"Result\"]\n",
    "    #st.dataframe(res_df)\n",
    "\n",
    "    # Resultado por pregunta\n",
    "    result_df = result.to_pandas()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 42/42 [00:16<00:00,  2.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the net income of 1st Source Corporat...</td>\n",
       "      <td>In 2019, 1st Source Corporation's net income r...</td>\n",
       "      <td>[2019 net income was $91.96 million compared t...</td>\n",
       "      <td>The net income of 1st Source Corporation in 20...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.961423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.843516</td>\n",
       "      <td>0.974066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the total interest income for 1st Sou...</td>\n",
       "      <td>1st Source Corporation raked in $282.8 million...</td>\n",
       "      <td>[Total interest income in 2019 was $282.877 , ...</td>\n",
       "      <td>The total interest income for 1st Source Corpo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.983839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740970</td>\n",
       "      <td>0.963879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What were the total deposits at the end of 201...</td>\n",
       "      <td>Total deposits held by 1st Source Corporation ...</td>\n",
       "      <td>[At year-end, total assets were $6.62 billion,...</td>\n",
       "      <td>At the end of 2019, total deposits were $5.36 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.875316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973539</td>\n",
       "      <td>0.894156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the amount of total loans and leases ...</td>\n",
       "      <td>The combined value of loans and leases that 1s...</td>\n",
       "      <td>[At year-end, total assets were $6.62 billion,...</td>\n",
       "      <td>The amount of total loans and leases outstandi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.915475</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.726280</td>\n",
       "      <td>0.905147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What were the net charge-offs in 2019 and how ...</td>\n",
       "      <td>In 2019, 1st Source Corporation achieved a sub...</td>\n",
       "      <td>[Net charge-offs (recoveries) were $5,048,000 ...</td>\n",
       "      <td>Net charge-offs in 2019 were $5,048,000, compa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.914138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526317</td>\n",
       "      <td>0.905246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What was the return on average assets (ROAA) f...</td>\n",
       "      <td>1st Source Corporation's ROAA rose to 1.41% in...</td>\n",
       "      <td>[Return on average assets (as a percent) for t...</td>\n",
       "      <td>The return on average assets (ROAA) for 2019 w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.857697</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.556876</td>\n",
       "      <td>0.894209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What was the efficiency ratio in 2019, and how...</td>\n",
       "      <td>2019 income: Non-interest income ($101.13M) vs...</td>\n",
       "      <td>[The efficiency ratio in 2019 is not explicitl...</td>\n",
       "      <td>Noninterest income was $101,130,000 in 2019, n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666208</td>\n",
       "      <td>0.950600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What was the net income of 1st Source Corporat...   \n",
       "1  What was the total interest income for 1st Sou...   \n",
       "2  What were the total deposits at the end of 201...   \n",
       "3  What was the amount of total loans and leases ...   \n",
       "4  What were the net charge-offs in 2019 and how ...   \n",
       "5  What was the return on average assets (ROAA) f...   \n",
       "6  What was the efficiency ratio in 2019, and how...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  In 2019, 1st Source Corporation's net income r...   \n",
       "1  1st Source Corporation raked in $282.8 million...   \n",
       "2  Total deposits held by 1st Source Corporation ...   \n",
       "3  The combined value of loans and leases that 1s...   \n",
       "4  In 2019, 1st Source Corporation achieved a sub...   \n",
       "5  1st Source Corporation's ROAA rose to 1.41% in...   \n",
       "6  2019 income: Non-interest income ($101.13M) vs...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [2019 net income was $91.96 million compared t...   \n",
       "1  [Total interest income in 2019 was $282.877 , ...   \n",
       "2  [At year-end, total assets were $6.62 billion,...   \n",
       "3  [At year-end, total assets were $6.62 billion,...   \n",
       "4  [Net charge-offs (recoveries) were $5,048,000 ...   \n",
       "5  [Return on average assets (as a percent) for t...   \n",
       "6  [The efficiency ratio in 2019 is not explicitl...   \n",
       "\n",
       "                                        ground_truth  context_precision  \\\n",
       "0  The net income of 1st Source Corporation in 20...                1.0   \n",
       "1  The total interest income for 1st Source Corpo...                1.0   \n",
       "2  At the end of 2019, total deposits were $5.36 ...                1.0   \n",
       "3  The amount of total loans and leases outstandi...                1.0   \n",
       "4  Net charge-offs in 2019 were $5,048,000, compa...                1.0   \n",
       "5  The return on average assets (ROAA) for 2019 w...                1.0   \n",
       "6  Noninterest income was $101,130,000 in 2019, n...                1.0   \n",
       "\n",
       "   faithfulness  answer_relevancy  context_recall  answer_correctness  \\\n",
       "0          1.00          0.961423             1.0            0.843516   \n",
       "1          0.00          0.983839             1.0            0.740970   \n",
       "2          1.00          0.875316             1.0            0.973539   \n",
       "3          0.50          0.915475             1.0            0.726280   \n",
       "4          0.00          0.914138             1.0            0.526317   \n",
       "5          0.00          0.857697             0.5            0.556876   \n",
       "6          0.75          0.000000             0.5            0.666208   \n",
       "\n",
       "   answer_similarity  \n",
       "0           0.974066  \n",
       "1           0.963879  \n",
       "2           0.894156  \n",
       "3           0.905147  \n",
       "4           0.905246  \n",
       "5           0.894209  \n",
       "6           0.950600  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = get_evaluation()\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
